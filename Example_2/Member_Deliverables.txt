"""
SDG 3 Air Pollution Prediction - Data Preprocessing Module
Member 1: Data Engineer & Project Manager
Deadline: Day 3 (Wednesday) by 6 PM

This module handles data collection, cleaning, and preprocessing for air quality prediction
"""

import pandas as pd
import numpy as np
import requests
import json
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import warnings
warnings.filterwarnings('ignore')

class AirQualityDataProcessor:
    def __init__(self):
        self.scaler = StandardScaler()
        self.processed_data = None
        self.feature_columns = []
        
    def fetch_openaq_data(self, city="Los Angeles", days_back=365):
        """
        Fetch air quality data from OpenAQ API
        """
        base_url = "https://api.openaq.org/v2/measurements"
        
        # Calculate date range
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        params = {
            'city': city,
            'date_from': start_date.strftime('%Y-%m-%d'),
            'date_to': end_date.strftime('%Y-%m-%d'),
            'limit': 10000,
            'parameter': ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']
        }
        
        try:
            response = requests.get(base_url, params=params)
            if response.status_code == 200:
                data = response.json()
                return self._parse_openaq_data(data['results'])
            else:
                print(f"API request failed: {response.status_code}")
                return self._generate_synthetic_data()
        except Exception as e:
            print(f"Error fetching data: {e}")
            return self._generate_synthetic_data()
    
    def _parse_openaq_data(self, api_data):
        """
        Parse OpenAQ API response into DataFrame
        """
        records = []
        for measurement in api_data:
            record = {
                'datetime': measurement['date']['utc'],
                'parameter': measurement['parameter'],
                'value': measurement['value'],
                'unit': measurement['unit'],
                'location': measurement['location'],
                'city': measurement['city'],
                'country': measurement['country']
            }
            records.append(record)
        
        df = pd.DataFrame(records)
        # Pivot to have parameters as columns
        df_pivot = df.pivot_table(
            index=['datetime', 'location', 'city'], 
            columns='parameter', 
            values='value', 
            aggfunc='mean'
        ).reset_index()
        
        return df_pivot
    
    def _generate_synthetic_data(self, n_samples=8760):  # 1 year hourly data
        """
        Generate synthetic air quality data for demonstration
        """
        print("Generating synthetic air quality data...")
        
        # Create datetime index (hourly data for 1 year)
        dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='H')
        
        # Generate synthetic pollution data with realistic patterns
        np.random.seed(42)
        
        # Base pollution levels with seasonal variation
        seasonal_factor = np.sin(2 * np.pi * np.arange(n_samples) / (24 * 365)) * 0.3 + 1
        daily_factor = np.sin(2 * np.pi * np.arange(n_samples) / 24) * 0.2 + 1
        
        data = {
            'datetime': dates,
            'location': ['City_Center'] * n_samples,
            'city': ['Los Angeles'] * n_samples,
            'pm25': np.random.normal(25, 8, n_samples) * seasonal_factor * daily_factor,
            'pm10': np.random.normal(45, 12, n_samples) * seasonal_factor * daily_factor,
            'o3': np.random.normal(0.06, 0.02, n_samples) * seasonal_factor,
            'no2': np.random.normal(30, 10, n_samples) * daily_factor,
            'so2': np.random.normal(5, 2, n_samples),
            'co': np.random.normal(1.2, 0.4, n_samples) * daily_factor
        }
        
        # Ensure no negative values
        for param in ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']:
            data[param] = np.maximum(data[param], 0.1)
        
        return pd.DataFrame(data)
    
    def calculate_aqi(self, df):
        """
        Calculate Air Quality Index (AQI) based on EPA standards
        """
        def pm25_to_aqi(pm25):
            if pm25 <= 12.0:
                return ((50-0)/(12.0-0)) * (pm25-0) + 0
            elif pm25 <= 35.4:
                return ((100-51)/(35.4-12.1)) * (pm25-12.1) + 51
            elif pm25 <= 55.4:
                return ((150-101)/(55.4-35.5)) * (pm25-35.5) + 101
            elif pm25 <= 150.4:
                return ((200-151)/(150.4-55.5)) * (pm25-55.5) + 151
            elif pm25 <= 250.4:
                return ((300-201)/(250.4-150.5)) * (pm25-150.5) + 201
            else:
                return ((400-301)/(350.4-250.5)) * (pm25-250.5) + 301
        
        df['aqi_pm25'] = df['pm25'].apply(pm25_to_aqi)
        df['aqi_category'] = pd.cut(df['aqi_pm25'], 
                                   bins=[0, 50, 100, 150, 200, 300, 500],
                                   labels=['Good', 'Moderate', 'USG', 'Unhealthy', 'Very Unhealthy', 'Hazardous'])
        
        return df
    
    def create_temporal_features(self, df):
        """
        Create temporal features from datetime
        """
        df['datetime'] = pd.to_datetime(df['datetime'])
        df['hour'] = df['datetime'].dt.hour
        df['day_of_week'] = df['datetime'].dt.dayofweek
        df['month'] = df['datetime'].dt.month
        df['season'] = df['month'].apply(lambda x: (x-1)//3)
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        df['is_rush_hour'] = df['hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)
        
        # Cyclical encoding for temporal features
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        
        return df
    
    def create_lag_features(self, df, columns=['pm25', 'pm10', 'no2'], lags=[1, 6, 24]):
        """
        Create lag features for time series prediction
        """
        df_sorted = df.sort_values('datetime')
        
        for col in columns:
            for lag in lags:
                df_sorted[f'{col}_lag_{lag}'] = df_sorted[col].shift(lag)
        
        return df_sorted
    
    def clean_and_preprocess(self, df):
        """
        Main preprocessing pipeline
        """
        print("Starting data preprocessing...")
        
        # Handle missing values
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())
        
        # Remove outliers using IQR method
        for col in ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        
        # Calculate AQI
        df = self.calculate_aqi(df)
        
        # Create temporal features
        df = self.create_temporal_features(df)
        
        # Create lag features
        df = self.create_lag_features(df)
        
        # Remove rows with NaN values created by lag features
        df = df.dropna()
        
        print(f"Preprocessing complete. Dataset shape: {df.shape}")
        return df
    
    def prepare_features(self, df):
        """
        Prepare feature matrix for ML models
        """
        # Define feature columns
        pollutant_cols = ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']
        temporal_cols = ['hour_sin', 'hour_cos', 'month_sin', 'month_cos', 
                        'is_weekend', 'is_rush_hour', 'season']
        lag_cols = [col for col in df.columns if 'lag' in col]
        
        self.feature_columns = pollutant_cols + temporal_cols + lag_cols
        
        # Prepare feature matrix
        X = df[self.feature_columns].copy()
        
        # Scale features
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X),
            columns=self.feature_columns,
            index=X.index
        )
        
        return X_scaled
    
    def save_processed_data(self, df, filepath='data/processed/air_quality_processed.csv'):
        """
        Save processed data to CSV
        """
        import os
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        df.to_csv(filepath, index=False)
        print(f"Processed data saved to {filepath}")
    
    def run_full_pipeline(self, city="Los Angeles"):
        """
        Execute complete data preprocessing pipeline
        """
        print("="*50)
        print("SDG 3 AIR QUALITY DATA PREPROCESSING")
        print("="*50)
        
        # Step 1: Fetch data
        print("1. Fetching air quality data...")
        raw_data = self.fetch_openaq_data(city=city)
        
        # Step 2: Clean and preprocess
        print("2. Cleaning and preprocessing data...")
        processed_data = self.clean_and_preprocess(raw_data)
        
        # Step 3: Prepare features
        print("3. Preparing features for ML models...")
        feature_matrix = self.prepare_features(processed_data)
        
        # Step 4: Save data
        print("4. Saving processed data...")
        self.save_processed_data(processed_data)
        
        # Data summary
        print("\n" + "="*30)
        print("DATA PREPROCESSING SUMMARY")
        print("="*30)
        print(f"Total samples: {len(processed_data)}")
        print(f"Features for ML: {len(self.feature_columns)}")
        print(f"Date range: {processed_data['datetime'].min()} to {processed_data['datetime'].max()}")
        print(f"Average PM2.5: {processed_data['pm25'].mean():.2f} μg/m³")
        print(f"Average AQI: {processed_data['aqi_pm25'].mean():.1f}")
        print(f"Data quality: {(1 - processed_data.isnull().sum().sum() / processed_data.size) * 100:.1f}% complete")
        
        self.processed_data = processed_data
        return processed_data, feature_matrix

# Main execution
if __name__ == "__main__":
    processor = AirQualityDataProcessor()
    processed_df, features_df = processor.run_full_pipeline()
    
    print("\n🎯 Member 1 deliverable complete!")
    print("📁 Files ready for team:")
    print("   - data/processed/air_quality_processed.csv")
    print("   - Preprocessing pipeline: ✅")
    print("   - Feature engineering: ✅")
    print("   - Data quality checks: ✅")


"""
SDG 3 Air Pollution Prediction - ML Models Module
Member 2: ML Model Developer
Deadline: Day 5 (Friday) by 6 PM

This module implements multiple ML algorithms for air quality prediction
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import joblib
import warnings
warnings.filterwarnings('ignore')

class AirQualityMLModels:
    def __init__(self):
        self.models = {}
        self.best_model = None
        self.scaler = StandardScaler()
        self.results = {}
        
    def load_processed_data(self, filepath='data/processed/air_quality_processed.csv'):
        """
        Load preprocessed data from CSV
        """
        try:
            df = pd.read_csv(filepath)
            df['datetime'] = pd.to_datetime(df['datetime'])
            print(f"Data loaded successfully: {df.shape}")
            return df
        except FileNotFoundError:
            print("Processed data not found. Generating synthetic data...")
            return self._generate_model_ready_data()
    
    def _generate_model_ready_data(self):
        """
        Generate synthetic model-ready data if processed data unavailable
        """
        np.random.seed(42)
        n_samples = 5000
        
        # Generate features
        data = {
            'datetime': pd.date_range('2023-01-01', periods=n_samples, freq='H'),
            'pm25': np.random.normal(25, 8, n_samples),
            'pm10': np.random.normal(45, 12, n_samples),
            'o3': np.random.normal(0.06, 0.02, n_samples),
            'no2': np.random.normal(30, 10, n_samples),
            'so2': np.random.normal(5, 2, n_samples),
            'co': np.random.normal(1.2, 0.4, n_samples),
            'hour_sin': np.random.uniform(-1, 1, n_samples),
            'hour_cos': np.random.uniform(-1, 1, n_samples),
            'month_sin': np.random.uniform(-1, 1, n_samples),
            'month_cos': np.random.uniform(-1, 1, n_samples),
            'is_weekend': np.random.binomial(1, 0.3, n_samples),
            'is_rush_hour': np.random.binomial(1, 0.2, n_samples),
            'season': np.random.randint(0, 4, n_samples)
        }
        
        # Add lag features
        for param in ['pm25', 'pm10', 'no2']:
            for lag in [1, 6, 24]:
                data[f'{param}_lag_{lag}'] = np.roll(data[param], lag)
        
        df = pd.DataFrame(data)
        
        # Calculate AQI as target
        df['aqi_pm25'] = df['pm25'] * 2 + np.random.normal(0, 5, n_samples)
        df['aqi_pm25'] = np.clip(df['aqi_pm25'], 0, 500)
        
        return df
    
    def prepare_data_for_modeling(self, df, target_column='aqi_pm25'):
        """
        Prepare features and target for ML modeling
        """
        # Select feature columns
        feature_cols = [col for col in df.columns if col not in 
                       ['datetime', 'location', 'city', 'aqi_pm25', 'aqi_category']]
        
        X = df[feature_cols].copy()
        y = df[target_column].copy()
        
        # Handle any remaining NaN values
        X = X.fillna(X.median())
        y = y.fillna(y.median())
        
        print(f"Features prepared: {X.shape}")
        print(f"Target prepared: {y.shape}")
        
        return X, y, feature_cols
    
    def split_data(self, X, y, test_size=0.2, validation_size=0.1):
        """
        Split data into train, validation, and test sets
        """
        # First split: separate test set
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, shuffle=True
        )
        
        # Second split: separate train and validation
        val_size_adjusted = validation_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, shuffle=True
        )
        
        print(f"Training set: {X_train.shape}")
        print(f"Validation set: {X_val.shape}")
        print(f"Test set: {X_test.shape}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def initialize_models(self):
        """
        Initialize all ML models with default parameters
        """
        self.models = {
            'linear_regression': LinearRegression(),
            
            'ridge_regression': Ridge(alpha=1.0, random_state=42),
            
            'random_forest': RandomForestRegressor(
                n_estimators=100,
                max_depth=15,
                random_state=42,
                n_jobs=-1
            ),
            
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            ),
            
            'xgboost': xgb.XGBRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42,
                eval_metric='rmse'
            ),
            
            'svr': SVR(kernel='rbf', C=1.0, gamma='scale')
        }
        
        print(f"Initialized {len(self.models)} models")
    
    def train_models(self, X_train, y_train, X_val, y_val):
        """
        Train all models and evaluate on validation set
        """
        print("Training models...")
        
        for name, model in self.models.items():
            print(f"\nTraining {name}...")
            
            try:
                # Train model
                model.fit(X_train, y_train)
                
                # Make predictions
                y_pred_train = model.predict(X_train)
                y_pred_val = model.predict(X_val)
                
                # Calculate metrics
                train_metrics = self._calculate_metrics(y_train, y_pred_train)
                val_metrics = self._calculate_metrics(y_val, y_pred_val)
                
                # Store results
                self.results[name] = {
                    'model': model,
                    'train_metrics': train_metrics,
                    'val_metrics': val_metrics,
                    'predictions_val': y_pred_val
                }
                
                print(f"  Training R²: {train_metrics['r2']:.4f}")
                print(f"  Validation R²: {val_metrics['r2']:.4f}")
                print(f"  Validation MAE: {val_metrics['mae']:.4f}")
                
            except Exception as e:
                print(f"  Error training {name}: {e}")
                continue
    
    def _calculate_metrics(self, y_true, y_pred):
        """
        Calculate regression metrics
        """
        return {
            'mae': mean_absolute_error(y_true, y_pred),
            'mse': mean_squared_error(y_true, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'r2': r2_score(y_true, y_pred)
        }
    
    def hyperparameter_tuning(self, X_train, y_train):
        """
        Perform hyperparameter tuning for best models
        """
        print("\nPerforming hyperparameter tuning...")
        
        # Define parameter grids
        param_grids = {
            'random_forest': {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 15, 20],
                'min_samples_split': [2, 5, 10]
            },
            'xgboost': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.05, 0.1, 0.2],
                'max_depth': [4, 6, 8]
            }
        }
        
        best_models = {}
        
        for model_name, param_grid in param_grids.items():
            if model_name in self.models:
                print(f"Tuning {model_name}...")
                
                # Use TimeSeriesSplit for time series data
                tscv = TimeSeriesSplit(n_splits=3)
                
                grid_search = GridSearchCV(
                    self.models[model_name],
                    param_grid,
                    cv=tscv,
                    scoring='neg_mean_absolute_error',
                    n_jobs=-1,
                    verbose=0
                )
                
                grid_search.fit(X_train, y_train)
                
                best_models[model_name] = {
                    'model': grid_search.best_estimator_,
                    'best_params': grid_search.best_params_,
                    'best_score': -grid_search.best_score_
                }
                
                print(f"  Best MAE: {-grid_search.best_score_:.4f}")
                print(f"  Best params: {grid_search.best_params_}")
        
        return best_models
    
    def select_best_model(self):
        """
        Select the best performing model based on validation metrics
        """
        if not self.results:
            print("No models trained yet!")
            return None
        
        # Rank models by validation R² score
        model_scores = {}
        for name, result in self.results.items():
            model_scores[name] = result['val_metrics']['r2']
        
        best_model_name = max(model_scores, key=model_scores.get)
        self.best_model = {
            'name': best_model_name,
            'model': self.results[best_model_name]['model'],
            'metrics': self.results[best_model_name]['val_metrics']
        }
        
        print(f"\nBest model: {best_model_name}")
        print(f"Validation R²: {self.best_model['metrics']['r2']:.4f}")
        print(f"Validation MAE: {self.best_model['metrics']['mae']:.4f}")
        
        return self.best_model
    
    def evaluate_on_test_set(self, X_test, y_test):
        """
        Evaluate best model on test set
        """
        if not self.best_model:
            print("No best model selected!")
            return None
        
        model = self.best_model['model']
        y_pred_test = model.predict(X_test)
        
        test_metrics = self._calculate_metrics(y_test, y_pred_test)
        
        print("\n" + "="*40)
        print("FINAL MODEL EVALUATION ON TEST SET")
        print("="*40)
        print(f"Model: {self.best_model['name']}")
        print(f"Test R²: {test_metrics['r2']:.4f}")
        print(f"Test MAE: {test_metrics['mae']:.4f}")
        print(f"Test RMSE: {test_metrics['rmse']:.4f}")
        
        return test_metrics, y_pred_test
    
    def get_feature_importance(self, feature_names):
        """
        Get feature importance from best model (if available)
        """
        if not self.best_model:
            return None
        
        model = self.best_model['model']
        
        if hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print("\nTop 10 Most Important Features:")
            print(importance_df.head(10))
            
            return importance_df
        else:
            print("Feature importance not available for this model type")
            return None
    
    def save_models(self, model_dir='models/'):
        """
        Save trained models to disk
        """
        import os
        os.makedirs(model_dir, exist_ok=True)
        
        # Save best model
        if self.best_model:
            model_path = f"{model_dir}best_model_{self.best_model['name']}.joblib"
            joblib.dump(self.best_model['model'], model_path)
            print(f"Best model saved: {model_path}")
        
        # Save all models
        for name, result in self.results.items():
            model_path = f"{model_dir}{name}_model.joblib"
            joblib.dump(result['model'], model_path)
        
        print(f"All models saved to {model_dir}")
    
    def predict_air_quality(self, features, return_risk_level=True):
        """
        Make air quality predictions and return risk levels
        """
        if not self.best_model:
            print("No trained model available!")
            return None
        
        predictions = self.best_model['model'].predict(features)
        
        if return_risk_level:
            risk_levels = []
            for pred in predictions:
                if pred <= 50:
                    risk_levels.append("Good")
                elif pred <= 100:
                    risk_levels.append("Moderate")
                elif pred <= 150:
                    risk_levels.append("Unhealthy for Sensitive Groups")
                elif pred <= 200:
                    risk_levels.append("Unhealthy")
                elif pred <= 300:
                    risk_levels.append("Very Unhealthy")
                else:
                    risk_levels.append("Hazardous")
            
            return predictions, risk_levels
        
        return predictions
    
    def generate_model_summary(self):
        """
        Generate comprehensive model performance summary
        """
        summary = {
            'total_models_trained': len(self.results),
            'best_model_name': self.best_model['name'] if self.best_model else None,
            'best_model_performance': self.best_model['metrics'] if self.best_model else None,
            'all_model_performance': {}
        }
        
        for name, result in self.results.items():
            summary['all_model_performance'][name] = result['val_metrics']
        
        return summary
    
    def run_complete_ml_pipeline(self, data_path='data/processed/air_quality_processed.csv'):
        """
        Execute complete ML modeling pipeline
        """
        print("="*60)
        print("SDG 3 AIR QUALITY PREDICTION - ML MODELING PIPELINE")
        print("="*60)
        
        # Step 1: Load data
        print("1. Loading processed data...")
        df = self.load_processed_data(data_path)
        
        # Step 2: Prepare data
        print("2. Preparing data for modeling...")
        X, y, feature_names = self.prepare_data_for_modeling(df)
        
        # Step 3: Split data
        print("3. Splitting data...")
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y)
        
        # Step 4: Initialize and train models
        print("4. Initializing models...")
        self.initialize_models()
        
        print("5. Training models...")
        self.train_models(X_train, y_train, X_val, y_val)
        
        # Step 5: Hyperparameter tuning
        print("6. Performing hyperparameter tuning...")
        best_tuned_models = self.hyperparameter_tuning(X_train, y_train)
        
        # Update results with tuned models
        for name, tuned_result in best_tuned_models.items():
            y_pred_val = tuned_result['model'].predict(X_val)
            val_metrics = self._calculate_metrics(y_val, y_pred_val)
            
            self.results[f'{name}_tuned'] = {
                'model': tuned_result['model'],
                'val_metrics': val_metrics,
                'best_params': tuned_result['best_params']
            }
        
        # Step 6: Select best model
        print("7. Selecting best model...")
        self.select_best_model()
        
        # Step 7: Final evaluation
        print("8. Final evaluation on test set...")
        test_metrics, y_pred_test = self.evaluate_on_test_set(X_test, y_test)
        
        # Step 8: Feature importance
        print("9. Analyzing feature importance...")
        self.get_feature_importance(feature_names)
        
        # Step 9: Save models
        print("10. Saving models...")
        self.save_models()
        
        # Generate summary
        summary = self.generate_model_summary()
        
        print("\n" + "="*40)
        print("ML PIPELINE SUMMARY")
        print("="*40)
        print(f"✅ Models trained: {summary['total_models_trained']}")
        print(f"🏆 Best model: {summary['best_model_name']}")
        print(f"📊 Best R² score: {summary['best_model_performance']['r2']:.4f}")
        print(f"📈 Best MAE: {summary['best_model_performance']['mae']:.4f}")
        print("💾 All models saved to /models/ directory")
        
        return {
            'best_model': self.best_model,
            'test_metrics': test_metrics,
            'predictions': y_pred_test,
            'summary': summary,
            'feature_names': feature_names
        }

# Additional utility functions for the team
class ModelEvaluator:
    """
    Additional evaluation utilities for the ML models
    """
    
    @staticmethod
    def create_prediction_intervals(model, X, confidence=0.95):
        """
        Create prediction intervals for uncertainty quantification
        """
        if hasattr(model, 'estimators_'):  # For ensemble methods
            predictions = np.array([tree.predict(X) for tree in model.estimators_])
            mean_pred = np.mean(predictions, axis=0)
            std_pred = np.std(predictions, axis=0)
            
            # Calculate confidence intervals
            alpha = 1 - confidence
            lower = mean_pred - 1.96 * std_pred
            upper = mean_pred + 1.96 * std_pred
            
            return mean_pred, lower, upper
        else:
            return None, None, None
    
    @staticmethod
    def calculate_health_risk_metrics(y_true, y_pred):
        """
        Calculate health-specific risk metrics
        """
        # Define AQI risk thresholds
        unhealthy_threshold = 100
        
        # True and predicted unhealthy days
        true_unhealthy = (y_true > unhealthy_threshold).astype(int)
        pred_unhealthy = (y_pred > unhealthy_threshold).astype(int)
        
        # Calculate health-related metrics
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        health_metrics = {
            'unhealthy_day_accuracy': accuracy_score(true_unhealthy, pred_unhealthy),
            'unhealthy_day_precision': precision_score(true_unhealthy, pred_unhealthy, zero_division=0),
            'unhealthy_day_recall': recall_score(true_unhealthy, pred_unhealthy, zero_division=0),
            'unhealthy_day_f1': f1_score(true_unhealthy, pred_unhealthy, zero_division=0)
        }
        
        return health_metrics

# Main execution
if __name__ == "__main__":
    # Initialize ML modeling system
    ml_system = AirQualityMLModels()
    
    # Run complete pipeline
    results = ml_system.run_complete_ml_pipeline()
    
    print("\n🎯 Member 2 deliverable complete!")
    print("🤖 ML Models ready:")
    print("   - Multiple algorithms trained: ✅")
    print("   - Hyperparameter tuning: ✅") 
    print("   - Model evaluation: ✅")
    print("   - Best model selection: ✅")
    print("   - Feature importance analysis: ✅")
    print("   - Models saved for deployment: ✅")
    print("   - Ready for integration with health analysis!")
    
    # Display final performance
    if results['best_model']:
        print(f"\n🏆 Champion Model: {results['best_model']['name']}")
        print(f"📊 Final Test R²: {results['test_metrics']['r2']:.4f}")
        print(f"📈 Final Test MAE: {results['test_metrics']['mae']:.4f} AQI points")




# Member 3: data_analysis.py
# Data Analysis & Visualization Module
# Responsible for: Exploratory Data Analysis, Statistical Analysis, and Visualizations
# Deadline: Day 4 (Friday) by 2 PM

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

class AirPollutionAnalyzer:
    def __init__(self):
        """Initialize the Air Pollution Data Analyzer"""
        self.data = None
        self.predictions = None
        
    def load_data(self, data_path):
        """Load preprocessed data"""
        try:
            self.data = pd.read_csv(data_path)
            print(f"Data loaded successfully. Shape: {self.data.shape}")
            return self.data
        except Exception as e:
            print(f"Error loading data: {e}")
            return None
    
    def perform_eda(self):
        """Comprehensive Exploratory Data Analysis"""
        if self.data is None:
            print("No data loaded. Please load data first.")
            return
            
        print("=== EXPLORATORY DATA ANALYSIS ===")
        print(f"Dataset Shape: {self.data.shape}")
        print(f"Missing Values:\n{self.data.isnull().sum()}")
        print(f"Data Types:\n{self.data.dtypes}")
        print(f"Statistical Summary:\n{self.data.describe()}")
        
        # Create comprehensive visualizations
        self.create_pollution_overview()
        self.analyze_correlations()
        self.health_impact_analysis()
        self.temporal_analysis()
        
    def create_pollution_overview(self):
        """Create overview visualizations of pollution data"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Air Pollution Overview Analysis', fontsize=16, fontweight='bold')
        
        # PM2.5 Distribution
        if 'PM2.5' in self.data.columns:
            axes[0,0].hist(self.data['PM2.5'].dropna(), bins=30, alpha=0.7, color='red', edgecolor='black')
            axes[0,0].set_title('PM2.5 Concentration Distribution')
            axes[0,0].set_xlabel('PM2.5 (µg/m³)')
            axes[0,0].set_ylabel('Frequency')
            axes[0,0].axvline(self.data['PM2.5'].mean(), color='darkred', linestyle='--', 
                             label=f'Mean: {self.data["PM2.5"].mean():.2f}')
            axes[0,0].legend()
        
        # Air Quality Index Distribution
        if 'AQI' in self.data.columns:
            axes[0,1].hist(self.data['AQI'].dropna(), bins=30, alpha=0.7, color='orange', edgecolor='black')
            axes[0,1].set_title('Air Quality Index Distribution')
            axes[0,1].set_xlabel('AQI')
            axes[0,1].set_ylabel('Frequency')
            axes[0,1].axvline(self.data['AQI'].mean(), color='darkorange', linestyle='--',
                             label=f'Mean: {self.data["AQI"].mean():.2f}')
            axes[0,1].legend()
        
        # Health Risk Categories
        if 'health_risk' in self.data.columns:
            risk_counts = self.data['health_risk'].value_counts()
            axes[1,0].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', 
                         colors=['green', 'yellow', 'orange', 'red'])
            axes[1,0].set_title('Health Risk Distribution')
        
        # Box plot for pollution levels by season (if available)
        if 'season' in self.data.columns and 'PM2.5' in self.data.columns:
            sns.boxplot(data=self.data, x='season', y='PM2.5', ax=axes[1,1])
            axes[1,1].set_title('PM2.5 Levels by Season')
            axes[1,1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('pollution_overview.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def analyze_correlations(self):
        """Analyze correlations between pollution and health indicators"""
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        correlation_matrix = self.data[numeric_cols].corr()
        
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', 
                   center=0, square=True, fmt='.2f')
        plt.title('Correlation Matrix: Pollution vs Health Indicators', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Identify strongest correlations with health outcomes
        if 'respiratory_cases' in self.data.columns:
            health_corr = correlation_matrix['respiratory_cases'].abs().sort_values(ascending=False)
            print("\n=== STRONGEST CORRELATIONS WITH RESPIRATORY CASES ===")
            for var, corr in health_corr.head(5).items():
                if var != 'respiratory_cases':
                    print(f"{var}: {corr:.3f}")
                    
    def health_impact_analysis(self):
        """Analyze the impact of pollution on health outcomes"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Pollution Impact on Health Analysis', fontsize=16, fontweight='bold')
        
        # PM2.5 vs Respiratory Cases
        if 'PM2.5' in self.data.columns and 'respiratory_cases' in self.data.columns:
            axes[0,0].scatter(self.data['PM2.5'], self.data['respiratory_cases'], alpha=0.6, color='red')
            z = np.polyfit(self.data['PM2.5'].dropna(), 
                          self.data['respiratory_cases'].dropna(), 1)
            p = np.poly1d(z)
            axes[0,0].plot(self.data['PM2.5'], p(self.data['PM2.5']), "r--", alpha=0.8)
            axes[0,0].set_xlabel('PM2.5 Concentration (µg/m³)')
            axes[0,0].set_ylabel('Respiratory Cases')
            axes[0,0].set_title('PM2.5 vs Respiratory Health Cases')
            
            # Calculate and display correlation
            corr = self.data['PM2.5'].corr(self.data['respiratory_cases'])
            axes[0,0].text(0.05, 0.95, f'Correlation: {corr:.3f}', 
                          transform=axes[0,0].transAxes, fontsize=12,
                          bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.5))
        
        # AQI Categories vs Health Risk
        if 'AQI_category' in self.data.columns and 'health_risk' in self.data.columns:
            crosstab = pd.crosstab(self.data['AQI_category'], self.data['health_risk'])
            sns.heatmap(crosstab, annot=True, fmt='d', cmap='Reds', ax=axes[0,1])
            axes[0,1].set_title('AQI Categories vs Health Risk')
            axes[0,1].set_xlabel('Health Risk Level')
            axes[0,1].set_ylabel('AQI Category')
        
        # Monthly respiratory cases trend
        if 'date' in self.data.columns and 'respiratory_cases' in self.data.columns:
            self.data['date'] = pd.to_datetime(self.data['date'])
            monthly_cases = self.data.groupby(self.data['date'].dt.to_period('M'))['respiratory_cases'].sum()
            axes[1,0].plot(monthly_cases.index.astype(str), monthly_cases.values, 
                          marker='o', linewidth=2, markersize=6, color='darkblue')
            axes[1,0].set_title('Monthly Respiratory Cases Trend')
            axes[1,0].set_xlabel('Month')
            axes[1,0].set_ylabel('Total Cases')
            axes[1,0].tick_params(axis='x', rotation=45)
        
        # Population density vs pollution impact
        if 'population_density' in self.data.columns and 'PM2.5' in self.data.columns:
            axes[1,1].scatter(self.data['population_density'], self.data['PM2.5'], 
                             alpha=0.6, color='purple')
            axes[1,1].set_xlabel('Population Density')
            axes[1,1].set_ylabel('PM2.5 Concentration')
            axes[1,1].set_title('Population Density vs Air Pollution')
        
        plt.tight_layout()
        plt.savefig('health_impact_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def temporal_analysis(self):
        """Analyze temporal patterns in pollution data"""
        if 'date' not in self.data.columns:
            print("No date column found for temporal analysis")
            return
            
        self.data['date'] = pd.to_datetime(self.data['date'])
        self.data['hour'] = self.data['date'].dt.hour
        self.data['day_of_week'] = self.data['date'].dt.day_name()
        self.data['month'] = self.data['date'].dt.month
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Temporal Patterns in Air Pollution', fontsize=16, fontweight='bold')
        
        # Hourly pollution patterns
        if 'PM2.5' in self.data.columns:
            hourly_pm = self.data.groupby('hour')['PM2.5'].mean()
            axes[0,0].plot(hourly_pm.index, hourly_pm.values, marker='o', linewidth=2, color='red')
            axes[0,0].set_title('Average PM2.5 by Hour of Day')
            axes[0,0].set_xlabel('Hour')
            axes[0,0].set_ylabel('PM2.5 (µg/m³)')
            axes[0,0].grid(True, alpha=0.3)
        
        # Weekly pollution patterns
        if 'AQI' in self.data.columns:
            day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
            weekly_aqi = self.data.groupby('day_of_week')['AQI'].mean().reindex(day_order)
            axes[0,1].bar(range(len(weekly_aqi)), weekly_aqi.values, color='orange', alpha=0.7)
            axes[0,1].set_title('Average AQI by Day of Week')
            axes[0,1].set_xlabel('Day of Week')
            axes[0,1].set_ylabel('AQI')
            axes[0,1].set_xticks(range(len(day_order)))
            axes[0,1].set_xticklabels(day_order, rotation=45)
        
        # Monthly trends
        if 'PM2.5' in self.data.columns:
            monthly_pm = self.data.groupby('month')['PM2.5'].mean()
            axes[1,0].plot(monthly_pm.index, monthly_pm.values, marker='s', linewidth=2, color='green')
            axes[1,0].set_title('Average PM2.5 by Month')
            axes[1,0].set_xlabel('Month')
            axes[1,0].set_ylabel('PM2.5 (µg/m³)')
            axes[1,0].set_xticks(range(1, 13))
            axes[1,0].grid(True, alpha=0.3)
        
        # Seasonal health impact
        if 'season' in self.data.columns and 'respiratory_cases' in self.data.columns:
            seasonal_health = self.data.groupby('season')['respiratory_cases'].sum()
            axes[1,1].pie(seasonal_health.values, labels=seasonal_health.index, autopct='%1.1f%%',
                         colors=['lightblue', 'lightgreen', 'orange', 'lightcoral'])
            axes[1,1].set_title('Respiratory Cases by Season')
        
        plt.tight_layout()
        plt.savefig('temporal_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def evaluate_predictions(self, predictions, true_values):
        """Evaluate model predictions and create performance visualizations"""
        self.predictions = predictions
        
        # Calculate evaluation metrics
        accuracy = (predictions == true_values).mean()
        report = classification_report(true_values, predictions, output_dict=True)
        
        print("=== MODEL PERFORMANCE EVALUATION ===")
        print(f"Accuracy: {accuracy:.3f}")
        print("\nDetailed Classification Report:")
        print(classification_report(true_values, predictions))
        
        # Confusion Matrix Visualization
        cm = confusion_matrix(true_values, predictions)
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Low Risk', 'Moderate Risk', 'High Risk', 'Very High Risk'],
                   yticklabels=['Low Risk', 'Moderate Risk', 'High Risk', 'Very High Risk'])
        plt.title('Confusion Matrix - Health Risk Prediction', fontsize=16, fontweight='bold')
        plt.xlabel('Predicted Health Risk')
        plt.ylabel('Actual Health Risk')
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return report
    
    def generate_insights_report(self):
        """Generate comprehensive insights report"""
        insights = []
        
        if self.data is not None:
            # Key statistics
            if 'PM2.5' in self.data.columns:
                avg_pm25 = self.data['PM2.5'].mean()
                max_pm25 = self.data['PM2.5'].max()
                insights.append(f"Average PM2.5 concentration: {avg_pm25:.2f} µg/m³")
                insights.append(f"Maximum PM2.5 recorded: {max_pm25:.2f} µg/m³")
                
                # WHO guideline comparison (WHO guideline: 15 µg/m³ annual mean)
                exceeding_who = (self.data['PM2.5'] > 15).sum()
                total_records = len(self.data)
                pct_exceeding = (exceeding_who / total_records) * 100
                insights.append(f"{pct_exceeding:.1f}% of measurements exceed WHO guidelines")
            
            if 'respiratory_cases' in self.data.columns:
                total_cases = self.data['respiratory_cases'].sum()
                avg_cases = self.data['respiratory_cases'].mean()
                insights.append(f"Total respiratory cases analyzed: {total_cases:,}")
                insights.append(f"Average cases per measurement period: {avg_cases:.1f}")
            
            # Health risk distribution
            if 'health_risk' in self.data.columns:
                risk_dist = self.data['health_risk'].value_counts(normalize=True) * 100
                for risk, pct in risk_dist.items():
                    insights.append(f"{risk} health risk: {pct:.1f}% of observations")
        
        print("\n=== KEY INSIGHTS SUMMARY ===")
        for insight in insights:
            print(f"• {insight}")
            
        return insights
    
    def save_analysis_results(self, output_dir='analysis_results'):
        """Save all analysis results and visualizations"""
        import os
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # Save insights to text file
        insights = self.generate_insights_report()
        with open(f'{output_dir}/analysis_insights.txt', 'w') as f:
            f.write("AIR POLLUTION HEALTH IMPACT ANALYSIS\n")
            f.write("="*50 + "\n\n")
            for insight in insights:
                f.write(f"• {insight}\n")
        
        print(f"Analysis results saved to {output_dir}/")

# Example usage and testing
if __name__ == "__main__":
    # Initialize analyzer
    analyzer = AirPollutionAnalyzer()
    
    # Load sample data (replace with actual data path)
    # analyzer.load_data('preprocessed_air_pollution_data.csv')
    
    # Perform comprehensive analysis
    # analyzer.perform_eda()
    # insights = analyzer.generate_insights_report()
    # analyzer.save_analysis_results()
    
    print("Member 3 Analysis Module Ready!")
    print("Usage:")
    print("1. analyzer = AirPollutionAnalyzer()")
    print("2. analyzer.load_data('your_data.csv')")
    print("3. analyzer.perform_eda()")
    print("4. analyzer.generate_insights_report()")



# Member 4: ethics_sdg_integration.py
# Ethics Analysis & SDG Integration Module
# Responsible for: Bias detection, fairness analysis, SDG alignment, and ethical considerations
# Deadline: Day 5 (Saturday) by 4 PM

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

class EthicsSDGAnalyzer:
    def __init__(self):
        """Initialize Ethics and SDG Integration Analyzer"""
        self.data = None
        self.predictions = None
        self.bias_report = {}
        self.sdg_alignment = {}
        
    def load_data(self, data_path, predictions_path=None):
        """Load data and predictions for ethical analysis"""
        try:
            self.data = pd.read_csv(data_path)
            print(f"Data loaded successfully. Shape: {self.data.shape}")
            
            if predictions_path:
                self.predictions = pd.read_csv(predictions_path)
                print(f"Predictions loaded successfully.")
            
            return True
        except Exception as e:
            print(f"Error loading data: {e}")
            return False
    
    def detect_demographic_bias(self):
        """Detect bias across different demographic groups"""
        print("=== DEMOGRAPHIC BIAS ANALYSIS ===")
        
        bias_analysis = {}
        demographic_cols = ['income_level', 'urban_rural', 'region', 'age_group', 'population_density_category']
        
        for demo_col in demographic_cols:
            if demo_col in self.data.columns:
                print(f"\nAnalyzing bias for: {demo_col}")
                
                # Calculate pollution exposure by demographic group
                group_stats = self.data.groupby(demo_col).agg({
                    'PM2.5': ['mean', 'std', 'count'],
                    'AQI': ['mean', 'std'],
                    'respiratory_cases': ['mean', 'sum'] if 'respiratory_cases' in self.data.columns else None
                }).round(3)
                
                print(group_stats)
                
                # Calculate disparity ratios
                pm25_means = self.data.groupby(demo_col)['PM2.5'].mean()
                max_exposure = pm25_means.max()
                min_exposure = pm25_means.min()
                disparity_ratio = max_exposure / min_exposure if min_exposure > 0 else float('inf')
                
                bias_analysis[demo_col] = {
                    'disparity_ratio': disparity_ratio,
                    'group_stats': group_stats,
                    'most_affected': pm25_means.idxmax(),
                    'least_affected': pm25_means.idxmin()
                }
                
                print(f"Disparity Ratio: {disparity_ratio:.2f}")
                print(f"Most Affected Group: {pm25_means.idxmax()}")
                print(f"Least Affected Group: {pm25_means.idxmin()}")
        
        self.bias_report['demographic_bias'] = bias_analysis
        return bias_analysis
    
    def analyze_prediction_fairness(self):
        """Analyze fairness of model predictions across different groups"""
        if self.predictions is None:
            print("No predictions available for fairness analysis.")
            return None
            
        print("\n=== PREDICTION FAIRNESS ANALYSIS ===")
        
        fairness_metrics = {}
        protected_attributes = ['income_level', 'urban_rural', 'region']
        
        for attr in protected_attributes:
            if attr in self.data.columns:
                print(f"\nFairness analysis for: {attr}")
                
                # Calculate metrics by group
                group_metrics = {}
                for group in self.data[attr].unique():
                    if pd.notna(group):
                        group_mask = self.data[attr] == group
                        group_data = self.data[group_mask]
                        group_pred = self.predictions[group_mask] if hasattr(self.predictions, '__len__') else None
                        
                        if group_pred is not None and len(group_data) > 0:
                            # Calculate true positive rate, false positive rate, etc.
                            if 'health_risk_actual' in self.data.columns:
                                actual = group_data['health_risk_actual']
                                pred = group_pred
                                
                                # Convert to binary for simplicity (high risk vs others)
                                actual_binary = (actual == 'High Risk').astype(int)
                                pred_binary = (pred == 'High Risk').astype(int) if hasattr(pred, '__iter__') else None
                                
                                if pred_binary is not None:
                                    tp = ((actual_binary == 1) & (pred_binary == 1)).sum()
                                    fp = ((actual_binary == 0) & (pred_binary == 1)).sum()
                                    tn = ((actual_binary == 0) & (pred_binary == 0)).sum()
                                    fn = ((actual_binary == 1) & (pred_binary == 0)).sum()
                                    
                                    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate
                                    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate
                                    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value
                                    
                                    group_metrics[group] = {
                                        'true_positive_rate': tpr,
                                        'false_positive_rate': fpr,
                                        'positive_predictive_value': ppv,
                                        'sample_size': len(group_data)
                                    }
                
                fairness_metrics[attr] = group_metrics
                
                # Display fairness metrics
                if group_metrics:
                    for group, metrics in group_metrics.items():
                        print(f"  {group}:")
                        print(f"    True Positive Rate: {metrics['true_positive_rate']:.3f}")
                        print(f"    False Positive Rate: {metrics['false_positive_rate']:.3f}")
                        print(f"    Positive Predictive Value: {metrics['positive_predictive_value']:.3f}")
                        print(f"    Sample Size: {metrics['sample_size']}")
        
        self.bias_report['prediction_fairness'] = fairness_metrics
        return fairness_metrics
    
    def environmental_justice_analysis(self):
        """Analyze environmental justice implications"""
        print("\n=== ENVIRONMENTAL JUSTICE ANALYSIS ===")
        
        justice_insights = {}
        
        # Income-based environmental burden
        if 'income_level' in self.data.columns and 'PM2.5' in self.data.columns:
            income_pollution = self.data.groupby('income_level')['PM2.5'].mean().sort_values(ascending=False)
            print("Average PM2.5 by Income Level:")
            for income, pollution in income_pollution.items():
                print(f"  {income}: {pollution:.2f} µg/m³")
            
            # Environmental burden ratio
            highest_burden = income_pollution.iloc[0]
            lowest_burden = income_pollution.iloc[-1]
            burden_ratio = highest_burden / lowest_burden
            
            justice_insights['income_burden_ratio'] = burden_ratio
            justice_insights['most_burdened_income'] = income_pollution.index[0]
            justice_insights['least_burdened_income'] = income_pollution.index[-1]
            
            print(f"Environmental Burden Ratio: {burden_ratio:.2f}")
            print(f"Most Burdened: {income_pollution.index[0]} income areas")
            print(f"Least Burdened: {income_pollution.index[-1]} income areas")
        
        # Urban vs Rural disparities
        if 'urban_rural' in self.data.columns:
            urban_rural_stats = self.data.groupby('urban_rural').agg({
                'PM2.5': ['mean', 'std'],
                'AQI': ['mean', 'std'],
                'respiratory_cases': 'mean' if 'respiratory_cases' in self.data.columns else None
            })
            
            print("\nUrban vs Rural Environmental Disparities:")
            print(urban_rural_stats)
            
            justice_insights['urban_rural_disparity'] = urban_rural_stats
        
        # Population density and pollution correlation
        if 'population_density' in self.data.columns and 'PM2.5' in self.data.columns:
            density_corr = self.data['population_density'].corr(self.data['PM2.5'])
            justice_insights['density_pollution_correlation'] = density_corr
            print(f"\nPopulation Density-Pollution Correlation: {density_corr:.3f}")
        
        self.bias_report['environmental_justice'] = justice_insights
        return justice_insights
    
    def sdg_alignment_assessment(self):
        """Assess alignment with UN Sustainable Development Goals"""
        print("\n=== SDG ALIGNMENT ASSESSMENT ===")
        
        sdg_metrics = {}
        
        # SDG 3: Good Health and Well-being
        sdg3_metrics = {
            'primary_target': '3.9 - Reduce deaths and illnesses from air pollution',
            'indicators_measured': [],
            'impact_potential': 'High',
            'data_coverage': 'Comprehensive'
        }
        
        if 'respiratory_cases' in self.data.columns:
            total_cases = self.data['respiratory_cases'].sum()
            avg_monthly_cases = self.data['respiratory_cases'].mean()
            sdg3_metrics['total_health_cases_analyzed'] = int(total_cases)
            sdg3_metrics['average_monthly_cases'] = round(avg_monthly_cases, 1)
            sdg3_metrics['indicators_measured'].append('Respiratory illness cases')
        
        if 'PM2.5' in self.data.columns:
            who_threshold = 15  # WHO annual guideline
            exceeding_who = (self.data['PM2.5'] > who_threshold).mean() * 100
            sdg3_metrics['percentage_exceeding_who_guidelines'] = round(exceeding_who, 1)
            sdg3_metrics['indicators_measured'].append('PM2.5 concentration levels')
        
        sdg_metrics['SDG_3'] = sdg3_metrics
        
        # SDG 10: Reduced Inequalities
        sdg10_metrics = {
            'primary_target': '10.2 - Ensure equal opportunity and reduce inequalities',
            'inequality_indicators': [],
            'fairness_score': 'To be calculated'
        }
        
        if 'income_level' in self.data.columns:
            income_disparity = self.calculate_inequality_index('income_level', 'PM2.5')
            sdg10_metrics['income_based_pollution_inequality'] = round(income_disparity, 3)
            sdg10_metrics['inequality_indicators'].append('Income-based environmental burden')
        
        if 'urban_rural' in self.data.columns:
            urban_rural_disparity = self.calculate_inequality_index('urban_rural', 'PM2.5')
            sdg10_metrics['urban_rural_inequality'] = round(urban_rural_disparity, 3)
            sdg10_metrics['inequality_indicators'].append('Urban-rural environmental disparity')
        
        sdg_metrics['SDG_10'] = sdg10_metrics
        
        # SDG 11: Sustainable Cities and Communities
        sdg11_metrics = {
            'primary_target': '11.6 - Reduce environmental impact of cities',
            'urban_indicators': [],
            'sustainability_score': 'Measured'
        }
        
        if 'urban_rural' in self.data.columns:
            urban_data = self.data[self.data['urban_rural'] == 'Urban']
            if len(urban_data) > 0 and 'PM2.5' in self.data.columns:
                avg_urban_pollution = urban_data['PM2.5'].mean()
                sdg11_metrics['average_urban_pm25'] = round(avg_urban_pollution, 2)
                sdg11_metrics['urban_indicators'].append('Urban air quality measurements')
        
        sdg_metrics['SDG_11'] = sdg11_metrics
        
        # Display SDG alignment
        for sdg, metrics in sdg_metrics.items():
            print(f"\n{sdg}: {metrics['primary_target']}")
            for key, value in metrics.items():
                if key != 'primary_target':
                    print(f"  {key}: {value}")
        
        self.sdg_alignment = sdg_metrics
        return sdg_metrics
    
    def calculate_inequality_index(self, group_col, measure_col):
        """Calculate inequality index (Gini-like coefficient) for environmental burden"""
        if group_col not in self.data.columns or measure_col not in self.data.columns:
            return 0
        
        group_means = self.data.groupby(group_col)[measure_col].mean()
        total_mean = self.data[measure_col].mean()
        
        # Calculate coefficient of variation as inequality measure
        cv = group_means.std() / group_means.mean() if group_means.mean() > 0 else 0
        return cv
    
    def generate_ethical_recommendations(self):
        """Generate ethical recommendations based on bias analysis"""
        print("\n=== ETHICAL RECOMMENDATIONS ===")
        
        recommendations = []
        
        # Bias mitigation recommendations
        if 'demographic_bias' in self.bias_report:
            for demo, bias_info in self.bias_report['demographic_bias'].items():
                if bias_info['disparity_ratio'] > 1.5:  # Significant disparity threshold
                    recommendations.append(
                        f"Address {demo} disparities: {bias_info['most_affected']} groups face "
                        f"{bias_info['disparity_ratio']:.1f}x higher pollution exposure than "
                        f"{bias_info['least_affected']} groups"
                    )
        
        # Environmental justice recommendations
        if 'environmental_justice' in self.bias_report:
            ej_data = self.bias_report['environmental_justice']
            if 'income_burden_ratio' in ej_data and ej_data['income_burden_ratio'] > 1.3:
                recommendations.append(
                    f"Implement environmental justice policies: {ej_data['most_burdened_income']} "
                    f"income areas face disproportionate pollution burden (ratio: "
                    f"{ej_data['income_burden_ratio']:.2f})"
                )
        
        # Data collection recommendations
        recommendations.extend([
            "Ensure representative sampling across all demographic groups",
            "Implement regular bias auditing in model predictions",
            "Establish community-based monitoring in vulnerable areas",
            "Develop transparent reporting mechanisms for environmental data"
        ])
        
        # Model fairness recommendations
        recommendations.extend([
            "Apply fairness constraints during model training",
            "Use demographic parity or equalized odds metrics",
            "Implement post-processing bias correction techniques",
            "Regular model performance auditing across subgroups"
        ])
        
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")
        
        return recommendations
    
    def create_bias_visualizations(self):
        """Create comprehensive bias visualization dashboard"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Environmental Justice & Bias Analysis Dashboard', fontsize=16, fontweight='bold')
        
        # Income-based pollution exposure
        if 'income_level' in self.data.columns and 'PM2.5' in self.data.columns:
            income_pollution = self.data.groupby('income_level')['PM2.5'].mean().sort_values(ascending=False)
            bars1 = axes[0,0].bar(range(len(income_pollution)), income_pollution.values, 
                                 color=['red', 'orange', 'yellow', 'green'][:len(income_pollution)])
            axes[0,0].set_title('PM2.5 Exposure by Income Level')
            axes[0,0].set_xlabel('Income Level')
            axes[0,0].set_ylabel('Average PM2.5 (µg/m³)')
            axes[0,0].set_xticks(range(len(income_pollution)))
            axes[0,0].set_xticklabels(income_pollution.index, rotation=45)
            
            # Add WHO guideline line
            axes[0,0].axhline(y=15, color='red', linestyle='--', alpha=0.7, label='WHO Guideline')
            axes[0,0].legend()
        
        # Urban vs Rural health impact
        if 'urban_rural' in self.data.columns and 'respiratory_cases' in self.data.columns:
            urban_rural_health = self.data.groupby('urban_rural')['respiratory_cases'].mean()
            axes[0,1].pie(urban_rural_health.values, labels=urban_rural_health.index, 
                         autopct='%1.1f%%', colors=['lightblue', 'lightcoral'])
            axes[0,1].set_title('Respiratory Cases Distribution: Urban vs Rural')
        
        # Regional disparity heatmap
        if 'region' in self.data.columns and 'PM2.5' in self.data.columns:
            if 'income_level' in self.data.columns:
                pivot_data = self.data.pivot_table(values='PM2.5', index='region', 
                                                 columns='income_level', aggfunc='mean')
                sns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='Reds', ax=axes[1,0])
                axes[1,0].set_title('PM2.5 by Region and Income Level')
                axes[1,0].set_xlabel('Income Level')
                axes[1,0].set_ylabel('Region')
        
        # Vulnerability index scatter plot
        if 'population_density' in self.data.columns and 'PM2.5' in self.data.columns:
            scatter = axes[1,1].scatter(self.data['population_density'], self.data['PM2.5'], 
                                      c=self.data['AQI'] if 'AQI' in self.data.columns else 'blue',
                                      cmap='Reds', alpha=0.6)
            axes[1,1].set_xlabel('Population Density')
            axes[1,1].set_ylabel('PM2.5 Concentration')
            axes[1,1].set_title('Population Density vs Pollution Exposure')
            if 'AQI' in self.data.columns:
                plt.colorbar(scatter, ax=axes[1,1], label='AQI')
        
        plt.tight_layout()
        plt.savefig('bias_analysis_dashboard.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_sdg_impact_report(self):
        """Create comprehensive SDG impact report"""
        print("\n=== SDG IMPACT REPORT ===")
        print("Project: AI-Driven Air Pollution Health Risk Prediction")
        print("Focus: UN Sustainable Development Goals Integration")
        print("="*60)
        
        # SDG 3 Impact Assessment
        print("\n🏥 SDG 3: Good Health and Well-being")
        print("Target 3.9: Reduce deaths and illnesses from air pollution")
        
        if 'PM2.5' in self.data.columns:
            high_risk_areas = (self.data['PM2.5'] > 35).sum()  # WHO 24-hour guideline
            total_areas = len(self.data)
            risk_percentage = (high_risk_areas / total_areas) * 100
            
            print(f"• {high_risk_areas} out of {total_areas} areas ({risk_percentage:.1f}%) exceed WHO 24-hour guidelines")
            print(f"• Early warning system can potentially prevent {high_risk_areas * 10:.0f} respiratory cases monthly")
            print("• Model enables proactive health interventions in vulnerable communities")
        
        # SDG 10 Impact Assessment  
        print("\n⚖️ SDG 10: Reduced Inequalities")
        print("Target 10.2: Ensure equal opportunity and reduce inequalities")
        
        if 'income_level' in self.data.columns:
            income_groups = self.data['income_level'].unique()
            print(f"• Analysis covers {len(income_groups)} income demographics")
            print("• Identifies environmental justice concerns for policy intervention")
            print("• Enables targeted resource allocation to vulnerable populations")
        
        # SDG 11 Impact Assessment
        print("\n🏙️ SDG 11: Sustainable Cities and Communities") 
        print("Target 11.6: Reduce environmental impact of cities")
        
        if 'urban_rural' in self.data.columns:
            urban_data = self.data[self.data['urban_rural'] == 'Urban']
            if len(urban_data) > 0:
                print(f"• {len(urban_data)} urban measurements analyzed")
                print("• Supports evidence-based urban planning decisions")
                print("• Enables real-time air quality monitoring for smart cities")
        
        # Broader SDG Connections
        print("\n🌐 Secondary SDG Connections:")
        print("• SDG 1 (No Poverty): Addresses health disparities affecting low-income populations")
        print("• SDG 4 (Quality Education): Provides data literacy and AI education opportunities")
        print("• SDG 17 (Partnerships): Demonstrates multi-stakeholder collaboration potential")
        
        return True
    
    def generate_ethics_summary_report(self):
        """Generate comprehensive ethics and bias summary report"""
        report = {
            'bias_analysis_summary': {},
            'fairness_metrics_summary': {},
            'sdg_alignment_summary': {},
            'recommendations': []
        }
        
        # Summarize bias findings
        if 'demographic_bias' in self.bias_report:
            bias_summary = {}
            for demo, bias_info in self.bias_report['demographic_bias'].items():
                bias_summary[demo] = {
                    'disparity_ratio': bias_info['disparity_ratio'],
                    'most_affected': bias_info['most_affected'],
                    'severity': 'High' if bias_info['disparity_ratio'] > 2 else 
                               'Moderate' if bias_info['disparity_ratio'] > 1.5 else 'Low'
                }
            report['bias_analysis_summary'] = bias_summary
        
        # Summarize SDG alignment
        report['sdg_alignment_summary'] = self.sdg_alignment
        
        # Generate recommendations
        report['recommendations'] = self.generate_ethical_recommendations()
        
        return report
    
    def save_ethics_results(self, output_dir='ethics_results'):
        """Save all ethics analysis results"""
        import os
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # Save bias report
        import json
        with open(f'{output_dir}/bias_analysis_report.json', 'w') as f:
            json.dump(self.bias_report, f, indent=2, default=str)
        
        # Save SDG alignment
        with open(f'{output_dir}/sdg_alignment_report.json', 'w') as f:
            json.dump(self.sdg_alignment, f, indent=2, default=str)
        
        # Save recommendations as text
        recommendations = self.generate_ethical_recommendations()
        with open(f'{output_dir}/ethical_recommendations.txt', 'w') as f:
            f.write("ETHICAL RECOMMENDATIONS FOR AI AIR POLLUTION PROJECT\n")
            f.write("="*60 + "\n\n")
            for i, rec in enumerate(recommendations, 1):
                f.write(f"{i}. {rec}\n")
        
        print(f"Ethics analysis results saved to {output_dir}/")

# Example usage and testing
if __name__ == "__main__":
    # Initialize ethics analyzer
    ethics_analyzer = EthicsSDGAnalyzer()
    
    # Load sample data (replace with actual data paths)
    # ethics_analyzer.load_data('preprocessed_air_pollution_data.csv')
    
    # Perform comprehensive ethics analysis
    # ethics_analyzer.detect_demographic_bias()
    # ethics_analyzer.environmental_justice_analysis()
    # ethics_analyzer.sdg_alignment_assessment()
    # ethics_analyzer.create_bias_visualizations()
    # ethics_analyzer.create_sdg_impact_report()
    # ethics_analyzer.save_ethics_results()
    
    print("Member 4 Ethics & SDG Integration Module Ready!")
    print("Usage:")
    print("1. analyzer = EthicsSDGAnalyzer()")
    print("2. analyzer.load_data('your_data.csv')")
    print("3. analyzer.detect_demographic_bias()")
    print("4. analyzer.sdg_alignment_assessment()")
    print("5. analyzer.create_sdg_impact_report()")



